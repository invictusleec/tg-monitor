import json
import datetime
import re
from typing import Dict, Any, List, Optional

from telethon.sync import TelegramClient
from telethon.sessions import StringSession
from sqlalchemy.orm import Session
from sqlalchemy import or_, cast, String

from config import settings
from model import Message, engine, ChannelRule, create_tables

# ------------------------ è§„åˆ™ç¼“å­˜ä¸åˆ¤æ–­ ------------------------
RULES_CACHE = {}

def load_rules_cache():
    global RULES_CACHE
    try:
        with Session(engine) as session:
            rules = session.query(ChannelRule).filter_by(enabled=True).all()
            RULES_CACHE = {
                r.channel: {
                    'exclude_netdisks': set((r.exclude_netdisks or [])),
                    'exclude_keywords': [kw.lower() for kw in (r.exclude_keywords or []) if kw],
                    'exclude_tags': set((r.exclude_tags or [])),
                }
                for r in rules
            }
            print(f"âš™ï¸ å·²åŠ è½½è§„åˆ™ {len(RULES_CACHE)} æ¡")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½è§„åˆ™å¤±è´¥: {e}")

def should_drop_by_rules(channel: str, parsed: dict) -> bool:
    if not channel:
        return False
    rule = RULES_CACHE.get(channel)
    if not rule:
        return False
    links = parsed.get('links') or {}
    if links and rule['exclude_netdisks'] and (set(links.keys()) & rule['exclude_netdisks']):
        return True
    kws = rule['exclude_keywords']
    if kws:
        title = (parsed.get('title') or '').lower()
        desc = (parsed.get('description') or '').lower()
        for kw in kws:
            if kw and (kw in title or kw in desc):
                return True
    tags = set(parsed.get('tags') or [])
    if tags and rule['exclude_tags'] and (tags & rule['exclude_tags']):
        return True
    return False

# ------------------------ æ–‡æœ¬è§£æï¼ˆä¸ monitor.py ä¿æŒä¸€è‡´ï¼‰ ------------------------

def parse_message(text: str) -> Dict[str, Any]:
    lines = text.split('\n')
    title = ''
    description = ''
    links = {}
    tags = []
    source = ''
    channel = ''
    group = ''
    bot = ''
    current_section = None
    desc_lines = []

    netdisk_map = [
        (['quark', 'å¤¸å…‹'], 'å¤¸å…‹ç½‘ç›˜'),
        (['aliyundrive', 'aliyun', 'é˜¿é‡Œ', 'alipan'], 'é˜¿é‡Œäº‘ç›˜'),
        (['baidu', 'pan.baidu'], 'ç™¾åº¦ç½‘ç›˜'),
        (['115.com', '115ç½‘ç›˜', '115pan'], '115ç½‘ç›˜'),
        (['cloud.189', 'å¤©ç¿¼', '189.cn'], 'å¤©ç¿¼äº‘ç›˜'),
        (['123pan', '123.yun'], '123äº‘ç›˜'),
        (['ucdisk', 'ucç½‘ç›˜', 'ucloud', 'drive.uc.cn'], 'UCç½‘ç›˜'),
        (['xunlei', 'thunder', 'è¿…é›·'], 'è¿…é›·'),
    ]

    if lines and lines[0].strip():
        if lines[0].startswith('åç§°ï¼š'):
            title = lines[0].replace('åç§°ï¼š', '').strip()
        else:
            title = lines[0].strip()

    for idx, line in enumerate(lines[1:] if title else lines):
        line = line.strip()
        if not line:
            continue
        if line.startswith('ğŸ· æ ‡ç­¾ï¼š') or line.startswith('æ ‡ç­¾ï¼š'):
            tags.extend([tag.strip('#') for tag in line.replace('ğŸ· æ ‡ç­¾ï¼š', '').replace('æ ‡ç­¾ï¼š', '').split() if tag.strip('#')])
            continue
        if line.startswith('æè¿°ï¼š'):
            current_section = 'description'
            desc_lines.append(line.replace('æè¿°ï¼š', '').strip())
        elif line.startswith('é“¾æ¥ï¼š'):
            current_section = 'links'
            url = line.replace('é“¾æ¥ï¼š', '').strip()
            if not url:
                continue
            found = False
            for keys, name in netdisk_map:
                if any(k in url.lower() for k in keys):
                    links[name] = url
                    found = True
                    break
            if not found:
                links['å…¶ä»–'] = url
        elif line.startswith('ğŸ‰ æ¥è‡ªï¼š'):
            source = line.replace('ğŸ‰ æ¥è‡ªï¼š', '').strip()
        elif line.startswith('ğŸ“¢ é¢‘é“ï¼š'):
            channel = line.replace('ğŸ“¢ é¢‘é“ï¼š', '').strip()
        elif line.startswith('ğŸ‘¥ ç¾¤ç»„ï¼š'):
            group = line.replace('ğŸ‘¥ ç¾¤ç»„ï¼š', '').strip()
        elif line.startswith('ğŸ¤– æŠ•ç¨¿ï¼š'):
            bot = line.replace('ğŸ¤– æŠ•ç¨¿ï¼š', '').strip()
        elif current_section == 'description':
            desc_lines.append(line)
        else:
            desc_lines.append(line)

    desc_text = '\n'.join(desc_lines)
    pattern = re.compile(r'([\u4e00-\u9fa5A-Za-z0-9#]+)[ï¼š:](https?://[^\s]+)')
    matches = pattern.findall(desc_text)
    for key, url in matches:
        found = False
        for keys, name in netdisk_map:
            if any(k in url.lower() or k in key for k in keys):
                links[name] = url
                found = True
                break
        if not found:
            links[key.strip()] = url
    desc_text = pattern.sub('', desc_text)

    url_pattern = re.compile(r'(https?://[^\s]+)')
    for url in url_pattern.findall(desc_text):
        found = False
        for keys, name in netdisk_map:
            if any(k in url.lower() for k in keys):
                links[name] = url
                found = True
                break
        if not found:
            links['å…¶ä»–'] = url
    desc_text = url_pattern.sub('', desc_text)

    tag_pattern = re.compile(r'#([\u4e00-\u9fa5A-Za-z0-9_]+)')
    found_tags = tag_pattern.findall(desc_text)
    if found_tags:
        tags.extend(found_tags)
        desc_text = tag_pattern.sub('', desc_text)

    tags = list(set(tags))
    netdisk_names = ['å¤¸å…‹', 'è¿…é›·', 'ç™¾åº¦', 'UC', 'é˜¿é‡Œ', 'å¤©ç¿¼', '115', '123äº‘ç›˜']
    netdisk_name_pattern = re.compile(r'(' + '|'.join(netdisk_names) + r')')
    desc_text = netdisk_name_pattern.sub('', desc_text)

    desc_lines_final = [line for line in desc_text.strip().split('\n') if line.strip() and not re.fullmatch(r'[.ã€‚Â·ã€,ï¼Œ-]+', line.strip())]
    description = '\n'.join(desc_lines_final)

    return {
        'title': title,
        'description': description,
        'links': links,
        'tags': tags,
        'source': source,
        'channel': channel,
        'group_name': group,
        'bot': bot
    }

# ------------------------ è¦†ç›–å†™å…¥ï¼ˆä»¥é“¾æ¥ä¸ºå”¯ä¸€ï¼‰ï¼Œæ‰¹é‡ä¼˜åŒ– ------------------------

def build_existing_link_index(session: Session) -> Dict[str, int]:
    link_to_id: Dict[str, int] = {}
    q = session.query(Message.id, Message.links).filter(Message.links.isnot(None))
    for mid, links in q.yield_per(1000):
        try:
            for url in (links or {}).values():
                if url and isinstance(url, str):
                    link_to_id[url] = mid
        except Exception:
            continue
    return link_to_id

# ------------------------ å¯¼å‡ºå…¨éƒ¨å†å²åˆ° txtï¼ˆJSONLï¼‰ ------------------------

from telethon.tl.functions.channels import GetFullChannelRequest
from telethon import functions
from telethon.tl.types import PeerChannel, InputMessagesFilterUrl

def export_history_txt(output_path: str, no_comments: bool = False, url_only: bool = False, min_id: Optional[int] = None):
    api_id = settings.TELEGRAM_API_ID
    api_hash = settings.TELEGRAM_API_HASH
    # ä¼˜å…ˆä½¿ç”¨å•ç‹¬çš„å¯¼å‡ºä¼šè¯ä»¥é¿å…ä¸çº¿ä¸Šç›‘æ§/å…¶ä»–è¿›ç¨‹å†²çª
    string_session = None
    if getattr(settings, 'EXPORT_STRING_SESSION', None):
        string_session = settings.EXPORT_STRING_SESSION.strip()
        print('ğŸ” ä½¿ç”¨ EXPORT_STRING_SESSION è¿›è¡Œå¯¼å‡º')
    elif getattr(settings, 'STRING_SESSION', None):
        string_session = settings.STRING_SESSION.strip()
        print('ğŸ” ä½¿ç”¨ STRING_SESSION è¿›è¡Œå¯¼å‡ºï¼ˆå¦‚é‡ä¼šè¯å†²çªï¼Œè¯·æ”¹ç”¨ EXPORT_STRING_SESSIONï¼‰')
    if not string_session:
        raise RuntimeError("æœªé…ç½® EXPORT_STRING_SESSION æˆ– STRING_SESSIONï¼Œè¯·åœ¨ .env ä¸­è®¾ç½®å…¶ä¸­ä¸€ä¸ªåå†è¿è¡Œè¯¥è„šæœ¬")

    target_channel = 'bsbdbfjfjff'
    total = 0
    from telethon.errors.rpcerrorlist import AuthKeyDuplicatedError

    # æå–æ¶ˆæ¯ä¸­çš„æ‰€æœ‰ URLï¼ˆæ­£æ–‡/å®ä½“/æŒ‰é’®ï¼‰
    url_regex = re.compile(r'(https?://[^\s]+)')
    def extract_urls_from_message(m) -> List[str]:
        urls = set()
        try:
            content = getattr(m, 'raw_text', '') or ''
            for u in url_regex.findall(content):
                urls.add(u)
            # entities ä¸­çš„åµŒå…¥é“¾æ¥
            ents = getattr(m, 'entities', None)
            if ents:
                for ent in ents:
                    if hasattr(ent, 'url') and ent.url:
                        urls.add(ent.url)
            # æŒ‰é’®ä¸Šçš„é“¾æ¥
            btns = getattr(m, 'buttons', None)
            if btns:
                for row in btns:
                    for b in row:
                        if getattr(b, 'url', None):
                            urls.add(b.url)
        except Exception:
            pass
        return list(urls)

    with TelegramClient(StringSession(string_session), api_id, api_hash) as client, open(output_path, 'w', encoding='utf-8') as f:
        print(f"ğŸ“¤ æ­£åœ¨å¯¼å‡ºé¢‘é“ @{target_channel} çš„å…¨éƒ¨å†å²æ¶ˆæ¯åˆ° {output_path}ï¼ˆJSONLï¼Œä¸€è¡Œä¸€æ¡ï¼‰...")
        if url_only:
            print("âš¡ å·²å¯ç”¨å¿«é€Ÿç­›é€‰ï¼šä»…æ‹‰å–åŒ…å«URLçš„æ¶ˆæ¯ï¼ˆæœåŠ¡å™¨ç«¯è¿‡æ»¤ï¼‰")
        if no_comments:
            print("â­ å·²ç¦ç”¨è¯„è®ºæŠ“å–ï¼ŒåŠ é€Ÿå¯¼å‡º")
        if min_id:
            print(f"â†— ä»…å¯¼å‡ºæ¶ˆæ¯ID >= {min_id} çš„å¢é‡éƒ¨åˆ†")
 
        # ä»…åœ¨éœ€è¦æŠ“å–è¯„è®ºæ—¶è§£æè®¨è®ºç»„ä¸é¢‘é“å®ä½“
        discussion = None
        channel_entity = None
        if not no_comments:
            try:
                full = client(GetFullChannelRequest(target_channel))
                linked_id = getattr(getattr(full, 'full_chat', None), 'linked_chat_id', None)
                if linked_id:
                    try:
                        discussion = client.get_entity(PeerChannel(linked_id))
                        print("ğŸ§µ å·²æ£€æµ‹åˆ°é¢‘é“ç»‘å®šè®¨è®ºç»„ï¼Œè¯„è®ºå°†ä¸€å¹¶å¯¼å‡º")
                    except Exception as e:
                        print(f"âš ï¸ æ— æ³•è§£æè®¨è®ºç»„å®ä½“: {e}")
            except Exception as e:
                print(f"âš ï¸ è·å–é¢‘é“å®Œæ•´ä¿¡æ¯å¤±è´¥ï¼Œå¯èƒ½æ— æ³•å¯¼å‡ºè¯„è®ºï¼š{e}")
            try:
                channel_entity = client.get_entity(target_channel)
            except Exception:
                channel_entity = target_channel

        iter_kwargs = { 'reverse': True }
        if min_id:
            iter_kwargs['min_id'] = min_id
        if url_only:
            iter_kwargs['filter'] = InputMessagesFilterUrl()

        for msg in client.iter_messages(target_channel, **iter_kwargs):
            total += 1
            text = getattr(msg, 'message', None) or getattr(msg, 'raw_text', None) or ''

            # å¦‚æ¶ˆæ¯æç¤ºâ€œè¯„è®ºåŒºæŸ¥çœ‹â€æˆ–å­˜åœ¨è¯„è®ºæ•°é‡ï¼Œåˆ™æŠ“å–è®¨è®ºç»„ä¸­çš„å¯¹åº”å›å¤å¹¶åˆå¹¶æ–‡æœ¬ä¸é“¾æ¥
            need_fetch_comments = False
            if not no_comments:
                try:
                    if text and ("è¯„è®ºåŒº" in text or "è¯„è®ºåŒºæŸ¥çœ‹" in text or "èµ„æºè¯„è®ºåŒºæŸ¥çœ‹" in text):
                        need_fetch_comments = True
                    replies_meta = getattr(msg, 'replies', None)
                    if replies_meta and getattr(replies_meta, 'replies', 0) > 0:
                        need_fetch_comments = True
                except Exception:
                    pass

            combined_text = text
            comments_appended = 0
            if discussion is not None and need_fetch_comments:
                comment_chunks: List[str] = []
                comment_urls: set = set()
                try:
                    top_id = None
                    try:
                        dm = client(functions.messages.GetDiscussionMessageRequest(peer=channel_entity, msg_id=getattr(msg, 'id', None)))
                        msgs = getattr(dm, 'messages', []) or []
                        if msgs:
                            # ä¼˜å…ˆé€‰æ‹©"è®¨è®ºç»„"åŒä¸€ peer çš„æ¶ˆæ¯ä½œä¸ºä¸»é¢˜å¸– id
                            cand = None
                            for m_ in msgs:
                                peer = getattr(m_, 'peer_id', None)
                                if peer is not None:
                                    # Channel ç±»å‹è®¨è®ºç»„
                                    if hasattr(peer, 'channel_id') and hasattr(discussion, 'id') and peer.channel_id == getattr(discussion, 'id', None):
                                        cand = m_
                                        break
                                    # Chat ç±»å‹è®¨è®ºç»„
                                    if hasattr(peer, 'chat_id') and hasattr(discussion, 'id') and peer.chat_id == getattr(discussion, 'id', None):
                                        cand = m_
                                        break
                            if cand is None:
                                cand = msgs[0]
                            top_id = getattr(cand, 'id', None)
                    except Exception as e:
                        print(f"âš ï¸ è·å–è®¨è®ºä¸»é¢˜å¤±è´¥(id={getattr(msg, 'id', None)}): {e}")
                    if top_id:
                        for reply in client.iter_messages(discussion, reply_to=top_id):
                            rtext = getattr(reply, 'raw_text', '') or ''
                            if rtext:
                                comment_chunks.append(rtext)
                            for u in extract_urls_from_message(reply):
                                comment_urls.add(u)
                            comments_appended += 1
                        # å°†è¯„è®ºå†…å®¹åˆå¹¶åˆ°åŸå§‹æ–‡æœ¬ï¼Œç¡®ä¿ä¸‹æ¸¸è§£æåˆ°è¯„è®ºé‡Œçš„é“¾æ¥
                        if comment_chunks:
                            combined_text = (text + "\n\n" if text else "") + "\n".join(comment_chunks)
                    else:
                        print(f"âš ï¸ è·³è¿‡è¯„è®ºæŠ“å–ï¼Œæ— æ³•å®šä½è®¨è®ºä¸»é¢˜(id={getattr(msg, 'id', None)})")
                except Exception as e:
                    print(f"âš ï¸ è¯»å–è¯„è®ºå¤±è´¥(id={getattr(msg, 'id', None)}): {e}")

            dt = getattr(msg, 'date', None)
            data = {
                'id': getattr(msg, 'id', None),
                'date': (dt.isoformat() if isinstance(dt, datetime.datetime) else None),
                'text': combined_text,
            }
            f.write(json.dumps(data, ensure_ascii=False) + '\n')
            if total % 200 == 0:
                if comments_appended:
                    print(f"  Â· å·²å¯¼å‡º {total} æ¡ï¼ˆæœ¬æ‰¹å«è¯„è®º {comments_appended} æ¡ï¼‰...", flush=True)
                else:
                    print(f"  Â· å·²å¯¼å‡º {total} æ¡...", flush=True)
    print(f"âœ… å¯¼å‡ºå®Œæˆï¼Œå…± {total} æ¡ã€‚")

# ------------------------ ä» txt æ‰¹é‡å¯¼å…¥æ•°æ®åº“ï¼ˆåªå¯¼å…¥å«ç½‘ç›˜é“¾æ¥ï¼‰ï¼Œé“¾æ¥å”¯ä¸€è¦†ç›– ------------------------

def import_from_txt(input_path: str):
    create_tables()
    load_rules_cache()

    target_channel = 'bsbdbfjfjff'
    inserted = 0
    updated = 0
    skipped_non_netdisk = 0

    with Session(engine) as session:
        link_index = build_existing_link_index(session)
        print(f"ğŸ§© ç°æœ‰é“¾æ¥ç´¢å¼•è½½å…¥å®Œæˆï¼ˆ{len(link_index)} æ¡å”¯ä¸€é“¾æ¥ï¼‰")

        batch_add: List[Message] = []
        batch_ops = 0
        BATCH_SIZE = 200

        with open(input_path, 'r', encoding='utf-8') as f:
            for line_no, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue
                text = (obj.get('text') or '').strip()
                if not text:
                    continue

                parsed = parse_message(text)
                parsed['channel'] = target_channel
                # åªå¯¼å…¥â€œå…³äºç½‘ç›˜â€çš„æ¶ˆæ¯
                if not parsed.get('links'):
                    skipped_non_netdisk += 1
                    continue
                # è§„åˆ™è¿‡æ»¤
                if should_drop_by_rules(target_channel, parsed):
                    continue

                ts = None
                if obj.get('date'):
                    try:
                        ts = datetime.datetime.fromisoformat(obj['date'])
                    except Exception:
                        pass
                if not ts:
                    ts = datetime.datetime.utcnow()

                urls = set((parsed.get('links') or {}).values())
                target_id = None
                for u in urls:
                    if u in link_index:
                        target_id = link_index[u]
                        break

                if target_id:
                    # æ›´æ–°è·¯å¾„ï¼šåŠ è½½å¹¶è¦†ç›–
                    target = session.query(Message).get(target_id)
                    if target is not None:
                        target.timestamp = ts
                        target.title = parsed.get('title')
                        target.description = parsed.get('description')
                        target.links = parsed.get('links')
                        target.tags = parsed.get('tags')
                        target.source = parsed.get('source')
                        target.channel = parsed.get('channel')
                        target.group_name = parsed.get('group_name')
                        target.bot = parsed.get('bot')
                        updated += 1
                        # æ›´æ–°ç´¢å¼•ï¼šä½¿ç”¨æ–°é“¾æ¥é›†åˆæŒ‡å‘åŒä¸€ id
                        for u in urls:
                            link_index[u] = target.id
                    else:
                        # å¼‚å¸¸æƒ…å†µï¼šç´¢å¼•å­˜åœ¨ä½†æ‰¾ä¸åˆ°è®°å½•ï¼Œèµ°æ’å…¥
                        m = Message(timestamp=ts, created_at=ts, **parsed)
                        batch_add.append(m)
                        for u in urls:
                            link_index[u] = -1  # å ä½ï¼Œcommitåæ›´æ–°
                        inserted += 1
                else:
                    # æ’å…¥è·¯å¾„
                    m = Message(timestamp=ts, created_at=ts, **parsed)
                    batch_add.append(m)
                    for u in urls:
                        link_index[u] = -1
                    inserted += 1

                batch_ops += 1
                if batch_ops >= BATCH_SIZE:
                    session.add_all(batch_add)
                    session.commit()
                    # commit åï¼Œå¡«å……æ–°å¢è®°å½•çš„ id åˆ°ç´¢å¼•
                    for m in batch_add:
                        for u in (m.links or {}).values():
                            link_index[u] = m.id
                    batch_add.clear()
                    batch_ops = 0
                    print(f"  Â· è¿›åº¦ï¼šæ–°å¢ {inserted}ï¼Œæ›´æ–° {updated}ï¼Œè·³è¿‡éç½‘ç›˜ {skipped_non_netdisk}", flush=True)

        if batch_add:
            session.add_all(batch_add)
            session.commit()
            for m in batch_add:
                for u in (m.links or {}).values():
                    link_index[u] = m.id
            batch_add.clear()

    print(f"âœ… å¯¼å…¥å®Œæˆï¼šæ–°å¢ {inserted} æ¡ï¼Œè¦†ç›–æ›´æ–° {updated} æ¡ï¼Œè·³è¿‡éç½‘ç›˜ {skipped_non_netdisk} æ¡")

# ------------------------ ä¸»æµç¨‹ï¼šå…ˆå¯¼å‡ºå†å¯¼å…¥ ------------------------

def main():
    import sys
    export_path = 'export_bsbdbfjfjff_all.txt'
    # æ”¯æŒè‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
    if '--output' in sys.argv:
        try:
            idx = sys.argv.index('--output')
            export_path = sys.argv[idx + 1]
        except Exception:
            pass
    export_only = ('--export-only' in sys.argv)

    # å¿«é€Ÿå¯¼å‡ºå‚æ•°
    no_comments = ('--no-comments' in sys.argv) or ('--fast' in sys.argv)
    url_only = ('--url-only' in sys.argv) or ('--fast' in sys.argv)
    min_id: Optional[int] = None
    if '--min-id' in sys.argv:
        try:
            idx = sys.argv.index('--min-id')
            min_id = int(sys.argv[idx + 1])
        except Exception:
            pass

    export_history_txt(export_path, no_comments=no_comments, url_only=url_only, min_id=min_id)
    if not export_only:
            import_from_txt(export_path)

if __name__ == '__main__':
    main()